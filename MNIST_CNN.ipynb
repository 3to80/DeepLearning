{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/3to80/DeepLearning/blob/master/MNIST_CNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "rBeZLaZEc1T-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "QookLQvxiKzW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 파이썬 2와 파이썬 3 지원\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# 공통\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "\n",
        "# 일관된 출력을 위해 유사난수 초기화\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# 맷플롯립 설정\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# 한글출력\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 그림을 저장할 폴더\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"cnn\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-TwixJ6Abin",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0hnhEYf2iOcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tr57Ro0YEZTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build graph\n",
        " \n",
        " - **mnist** : 28 * 28 * 1\n",
        "\n",
        "- **cnn** \n",
        "  - conv : 64, [3, 3], 1 SAME\n",
        "  - pool1: 64, [3, 3], 2 SAME\n",
        "  - conv: 128, [3, 3], 1 SAME\n",
        "  - pool2: 128, [3,3], 2 SAME\n",
        "  - flat : tf.reshape(pool2, shape=[-1, conv2_fmaps * w/4 * h/4])\n",
        "  \n",
        "  \n",
        " - **dnn** \n",
        "  - dropout : 0.5 //  bn_momentum : 0.95\n",
        "  - layer1 : 128\n",
        "    - drop1\n",
        "    - fc1\n",
        "    - bn1\n",
        "    \n",
        "  - layer2 : 128\n",
        "    - drop2\n",
        "    - fc2\n",
        "    - bn2"
      ]
    },
    {
      "metadata": {
        "id": "t4I6aPgNiQ7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "546dadc0-21df-4a62-f6ba-18b672a54779"
      },
      "cell_type": "code",
      "source": [
        "height = 28\n",
        "width = 28\n",
        "\n",
        "\n",
        "channels = 1\n",
        "n_inputs = height * width\n",
        "\n",
        "conv1_fmaps = 32\n",
        "conv1_ksize = 3\n",
        "conv1_stride = 1\n",
        "conv1_pad = \"SAME\"\n",
        "\n",
        "\n",
        "pool1_fmaps = 32\n",
        "pool1_stride = 2\n",
        "pool1_pad = \"SAME\"\n",
        "# 14 14 \n",
        "\n",
        "\n",
        "conv2_fmaps = 64\n",
        "conv2_ksize = 3\n",
        "conv2_stride = 1\n",
        "conv2_pad = \"SAME\"\n",
        "#14 14\n",
        "\n",
        "n_outputs = 10\n",
        "\n",
        "reset_graph()\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "\n",
        "\n",
        "with tf.name_scope(\"inputs\"):\n",
        "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
        "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels], name=\"X_reshape\")\n",
        "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "\n",
        "## X_input : 128, 784, reshape : X_reshape : 128, 28, 28, 1 \n",
        "\n",
        "with tf.name_scope(\"cnn\"):    \n",
        "  # X_reshape : NONE, 28, 28, 1\n",
        "  print(\"인풋 shape \" , X_reshaped.shape)\n",
        "  \n",
        "  conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
        "                         strides=conv1_stride, padding=conv1_pad,\n",
        "                         activation=tf.nn.selu, name=\"conv1\")\n",
        "  # 28 28 conv1_fmaps\n",
        "  # NONE , 28 , 28 , 32 \n",
        "  print(\"conv1 shape \" , conv1.shape)\n",
        "\n",
        "  \n",
        "  pool1 = tf.nn.max_pool(conv1, ksize = [1,3,3,1], strides=[1,2,2,1], padding=\"SAME\")  \n",
        "  # 14 14 conv1_fmaps\n",
        "  # NONE , 14, 14, 32\n",
        "  print(\"pool1 shape \", pool1.shape)\n",
        "  \n",
        "  \n",
        "  conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
        "                         strides=conv2_stride, padding=conv2_pad,\n",
        "                         activation=tf.nn.selu, name=\"conv2\")\n",
        "  # NONE , 14, 14, 64\n",
        "  print(\"conv2 shape  \", conv2.shape)\n",
        "  \n",
        "  \n",
        "  pool2 = tf.nn.max_pool(conv2, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\")\n",
        "  \n",
        "  # NONE , 7, 7,64\n",
        "  print(\"pool2 shape \", pool2.shape)\n",
        "\n",
        "  \n",
        "  conv3_fmaps = 128\n",
        "  conv3_ksize = 3\n",
        "  conv3_stride = 1\n",
        "  conv3_pad = \"SAME\"\n",
        "  \n",
        "  conv3 = tf.layers.conv2d(pool2, filters=conv3_fmaps, kernel_size=conv3_ksize,\n",
        "                         strides=conv3_stride, padding=conv3_pad,\n",
        "                         activation=tf.nn.selu, name=\"conv3\")\n",
        "\n",
        "  \n",
        "  print(\"conv3 shape \", conv3.shape)\n",
        "  \n",
        "  flat = tf.reshape(conv3, shape=[-1, conv3_fmaps * 7 * 7])\n",
        "  print(\"flat shape \", flat.shape)\n",
        "\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "drop_out = 0.5\n",
        "\n",
        "n_fc1 = 128\n",
        "# n_fc2 = 128\n",
        "\n",
        "with tf.name_scope(\"fc\"):\n",
        "#   fc1 = tf.layers.dense(flat , n_fc1, activation=tf.nn.selu, name=\"fc1\")\n",
        "#   fc2 = tf.layers.dense(fc1, n_fc2, activation=tf.nn.selu, name=\"fc2\")\n",
        "  \n",
        "  dnn_inputs1 = tf.layers.dropout(flat, drop_out , training=training)\n",
        "  fc1 = tf.layers.dense(dnn_inputs1 , n_fc1, activation=tf.nn.selu, name=\"fc1\")\n",
        "#   bn1 = tf.layers.batch_normalization(fc1, momentum = 0.95, training = training)\n",
        "  \n",
        "#   fc1_output = tf.nn.selu(bn1)\n",
        "\n",
        "  \n",
        "#   dnn_inputs2 = tf.layers.dropout(fc1_output, drop_out , training=training)\n",
        "#   fc2 = tf.layers.dense(dnn_inputs2, n_fc2, activation=tf.nn.selu, name=\"fc2\")\n",
        "#   bn2 = tf.layers.batch_normalization(fc2, momentum = 0.95, training = training)\n",
        "\n",
        "  dnn_output = fc1\n",
        "\n",
        "  \n",
        "with tf.name_scope(\"output\"):\n",
        "    logits = tf.layers.dense(dnn_output, n_outputs, name=\"output\")\n",
        "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
        "\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)    \n",
        "    loss_summary = tf.summary.scalar(\"loss_summary\", loss)\n",
        "\n",
        "    \n",
        "with tf.name_scope(\"training\"):\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "    \n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    acc_summary = tf.summary.scalar(\"accuracy\", accuracy)    \n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "인풋 shape  (?, 28, 28, 1)\n",
            "conv1 shape  (?, 28, 28, 32)\n",
            "pool1 shape  (?, 14, 14, 32)\n",
            "conv2 shape   (?, 14, 14, 64)\n",
            "pool2 shape  (?, 7, 7, 64)\n",
            "conv3 shape  (?, 7, 7, 128)\n",
            "flat shape  (?, 6272)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cP40Bb4SG1oB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Mnist Data 준비(Train /Valid /Test ) "
      ]
    },
    {
      "metadata": {
        "id": "tn1EoID0D8WX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5426157c-352c-461f-cdf2-ad9f16d81c81"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)  # deprecated 경고 메세지를 출력하지 않기 위해\n",
        "mnist = input_data.read_data_sets(\"./tmp/data/\")\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "X_train = mnist.train.images\n",
        "y_train = mnist.train.labels\n",
        "\n",
        "X_valid = mnist.validation.images\n",
        "y_valid = mnist.validation.labels\n",
        "\n",
        "X_test = mnist.test.images\n",
        "y_test = mnist.test.labels\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9bUUA51YH8Oc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mnist label 준비\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eF3nyq6iICeO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# 훈련 세트로부터 n_inputs와 n_outputs를 구하기\n",
        "# 기존 label 값에 idx 를 붙이는거. 그러니까 0` 9 가 아닌  다른값일 때 값 정돈하는 방법\n",
        "classes = np.unique(y_train)\n",
        "# n_outputs = len(classes)\n",
        "\n",
        "class_to_index = {\n",
        "    label: index for index, label in enumerate(classes)\n",
        "}\n",
        "print(class_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTlUqcXGG9Lp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train \n",
        "  - epoch : 100\n",
        "  - batch_size : 128"
      ]
    },
    {
      "metadata": {
        "id": "V87hHiHmKHsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 조기 종료를 위한 var\n",
        "\n",
        "- max_checks_without_progress = 20\n",
        "- checks_without_progress = 0\n",
        "- best_loss = np.infty\n",
        "- best_params = None\n"
      ]
    },
    {
      "metadata": {
        "id": "yAiQyM_rnioc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Param 저장 및 복원\n",
        "\n",
        "- get_model_params()\n",
        "- set_model_params()"
      ]
    },
    {
      "metadata": {
        "id": "koOkyRRHniU0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {\n",
        "        gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DN-bEplAqWJf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mnist Fitting"
      ]
    },
    {
      "metadata": {
        "id": "h1imvOEAqUXy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "B6s2MkMYik9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "ae7c5941-d801-4e71-bd74-67d0b64203ff"
      },
      "cell_type": "code",
      "source": [
        "tf.summary.merge_all()\n",
        "from datetime import datetime\n",
        "# epochs 와 batch_Szie \n",
        "epochs = 100\n",
        "batch_size =128\n",
        "\n",
        "#best_param\n",
        "max_checks_without_progress = 20\n",
        "checks_without_progress = 0\n",
        "best_loss = np.infty\n",
        "best_params = None\n",
        "\n",
        "\n",
        "\n",
        "def log_dir(output_path, prefix=\"\"):\n",
        "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "    name = prefix + \"run-\" + now\n",
        "    return \"{}/{}/\".format(output_path, name)\n",
        "  \n",
        "file_writer = tf.summary.FileWriter(log_dir(\"board_log\"), graph=tf.get_default_graph() )\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    rnd_idx = np.random.permutation(len(X_train))\n",
        "    for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
        "      X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
        "      \n",
        "      feed_dict = {X : X_batch, y: y_batch}\n",
        "      if training is not None:\n",
        "        feed_dict[training] = True                    \n",
        "      \n",
        "      sess.run(training_op, feed_dict= feed_dict)\n",
        "    \n",
        "#       if extra_update_ops:\n",
        "#           sess.run(extra_update_ops, feed_dict=feed_dict)\n",
        "          \n",
        "    if epoch % 5 ==0 :\n",
        "      acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "\n",
        "      loss_val, acc_val, loss_smm, acc_smm = sess.run([loss, accuracy, loss_summary, acc_summary], \n",
        "                                                      feed_dict={X: X_valid, y: y_valid})\n",
        "      \n",
        "      if loss_val < best_loss:\n",
        "#         gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "#         best_params = {gvar.op.name: value for gvar, value in zip(gvars, sess.run(gvars))}\n",
        "        best_loss = loss_val\n",
        "        best_params= get_model_params()\n",
        "        checks_without_progress = 0\n",
        "      else:\n",
        "        checks_without_progress +=1 \n",
        "\n",
        "      print(epoch, \"훈련 정확도:\", acc_train, \"검증 세트 정확도:\", acc_val, \"검증 loss val: \", loss_val)\n",
        "      file_writer.add_summary(summary=acc_smm, global_step=epoch)\n",
        "      file_writer.add_summary(summary=loss_smm, global_step=epoch)\n",
        "      if checks_without_progress > max_checks_without_progress:\n",
        "        print(\"조기 종료!\")\n",
        "        break\n",
        "\n",
        "  \n",
        "  if best_params:  \n",
        "    restore_model_params(best_params)\n",
        "  acc_test = accuracy.eval(feed_dict={X: X_test,\n",
        "                                        y: y_test})\n",
        "  print(\"테스트 세트에서 최종 정확도:\", acc_test)\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 훈련 정확도: 0.9765625 검증 세트 정확도: 0.9848 검증 loss val:  0.05518033\n",
            "5 훈련 정확도: 1.0 검증 세트 정확도: 0.9914 검증 loss val:  0.038408432\n",
            "10 훈련 정확도: 1.0 검증 세트 정확도: 0.9874 검증 loss val:  0.058705695\n",
            "15 훈련 정확도: 1.0 검증 세트 정확도: 0.9936 검증 loss val:  0.04517377\n",
            "20 훈련 정확도: 1.0 검증 세트 정확도: 0.9906 검증 loss val:  0.06433026\n",
            "25 훈련 정확도: 0.9921875 검증 세트 정확도: 0.9904 검증 loss val:  0.07314846\n",
            "30 훈련 정확도: 1.0 검증 세트 정확도: 0.9918 검증 loss val:  0.07399049\n",
            "35 훈련 정확도: 1.0 검증 세트 정확도: 0.9908 검증 loss val:  0.11311185\n",
            "40 훈련 정확도: 1.0 검증 세트 정확도: 0.992 검증 loss val:  0.12022979\n",
            "45 훈련 정확도: 1.0 검증 세트 정확도: 0.9902 검증 loss val:  0.15862979\n",
            "50 훈련 정확도: 1.0 검증 세트 정확도: 0.9908 검증 loss val:  0.13868095\n",
            "55 훈련 정확도: 1.0 검증 세트 정확도: 0.9942 검증 loss val:  0.09519664\n",
            "60 훈련 정확도: 1.0 검증 세트 정확도: 0.992 검증 loss val:  0.13435438\n",
            "65 훈련 정확도: 1.0 검증 세트 정확도: 0.9932 검증 loss val:  0.18239403\n",
            "70 훈련 정확도: 1.0 검증 세트 정확도: 0.9936 검증 loss val:  0.13058555\n",
            "75 훈련 정확도: 1.0 검증 세트 정확도: 0.9928 검증 loss val:  0.17161426\n",
            "80 훈련 정확도: 1.0 검증 세트 정확도: 0.993 검증 loss val:  0.22695093\n",
            "85 훈련 정확도: 1.0 검증 세트 정확도: 0.993 검증 loss val:  0.19028696\n",
            "90 훈련 정확도: 1.0 검증 세트 정확도: 0.993 검증 loss val:  0.25152454\n",
            "95 훈련 정확도: 1.0 검증 세트 정확도: 0.9918 검증 loss val:  0.27859294\n",
            "테스트 세트에서 최종 정확도: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}